{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WVZBScZLdbO6xnO9Mz7__lkA3NJNqDS6",
      "authorship_tag": "ABX9TyPj/I9st49va1Uin8ZGW4du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunpandey77/COVID-19/blob/master/Everything_For_Generative_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TABLE OF CONTENTS\n",
        "##1 BACKGROUND\n",
        "##2 CORE MATHEMATICS\n",
        "##3 DEEP LEARNING\n",
        "##4 NLP\n",
        "##5 GENERATIVE AI\n"
      ],
      "metadata": {
        "id": "szdzCG-LNB0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 BACKGROUND:\n",
        "Generative AI stands at the forefront of innovation, enabling machines to create content, mimic human-like behaviors, and generate remarkably authentic outputs. To delve into this realm, one must grasp an array of foundational technologies across various domains:\n",
        "\n",
        "## 1.1 Fundamental Mathematics Behind Generative AI (ML in General)\n",
        "Understanding the mathematical fundamentals is crucial for delving into generative AI. Here's a breakdown of key mathematical components:\n",
        "\n",
        "###Linear Algebra\n",
        "Vectors and Matrices: Fundamentals of representing data and transformations.\n",
        "\n",
        "Matrix Operations: Multiplication, addition, inversion, and decomposition for manipulating data.\n",
        "\n",
        "Eigenvalues and Eigenvectors: Understanding transformations and dimensionality reduction.\n",
        "\n",
        "###Calculus\n",
        "\n",
        "Differential Calculus: Rates of change, gradients, derivatives for optimizing functions.\n",
        "\n",
        "Integral Calculus: Area under curves, probability distributions, and accumulation.\n",
        "\n",
        "###Probability and Statistics\n",
        "Probability Theory: Understanding uncertainty, distributions, and randomness in data.\n",
        "\n",
        "Statistics: Descriptive statistics, hypothesis testing, and parameter estimation.\n",
        "\n",
        "###Optimization\n",
        "Gradient Descent: Optimizing models by adjusting parameters based on gradients.\n",
        "\n",
        "Convex Optimization: Techniques for finding global optima in convex problems.\n",
        "\n",
        "Stochastic Optimization: Dealing with noisy or incomplete information in optimization problems.\n",
        "\n",
        "###Information Theory\n",
        "Entropy: Measuring uncertainty and information content in data.\n",
        "\n",
        "KL Divergence: Quantifying differences between probability distributions.\n",
        "Algorithmic Foundations\n",
        "\n",
        "Graph Theory: Representing relationships between entities in data.\n",
        "Complexity Theory: Analyzing the efficiency and scalability of algorithms.\n",
        "\n",
        "##1.2 Deep Learning\n",
        "Deep learning methods are fundamental to many generative AI models:\n",
        "\n",
        "Neural Networks: Basic building blocks for modeling complex relationships.\n",
        "Feedforward Networks: Unidirectional information flow.\n",
        "Recurrent Neural Networks (RNNs): Handling sequential data.\n",
        "Convolutional Neural Networks (CNNs): Extracting spatial patterns.\n",
        "Autoencoders: Learning efficient representations of data.\n",
        "Generative Adversarial Networks (GANs): Generating realistic data.\n",
        "Variational Autoencoders (VAEs): Utilizing variational inference for learning latent representations.\n",
        "\n",
        "##1.3 Natural Language Processing (NLP)\n",
        "NLP forms the backbone for generative AI in language-related tasks:\n",
        "\n",
        "Tokenization: Breaking text into smaller units (words, subwords, characters).\n",
        "Word Embeddings: Representing words as vectors in a continuous space.\n",
        "Sequence Modeling: Handling sequences of words or tokens.\n",
        "Language Models: Predicting the probability of a word given the previous words.\n",
        "Text Generation: Creating coherent and contextually relevant text.\n",
        "\n",
        "##1.4 Generative AI Components\n",
        "Specific components pivotal to generative models:\n",
        "\n",
        "Transformer Architecture: Self-attention mechanism-based architecture crucial for sequence modeling tasks.\n",
        "\n",
        "Attention Mechanism: Focusing on specific parts of input when processing sequences.\n",
        "\n",
        "Softmax Activation: Assigning probabilities to a set of classes.\n",
        "\n",
        "LangChain: Chaining tokens together for coherent text generation.\n",
        "\n",
        "Retrieve-and-Generate (RAG): Merging retrieval and generation models for coherent responses.\n",
        "\n",
        "Reformer with Hierarchical Length-Factorization (RHLF): Enhancing efficiency in self-attention mechanisms within Transformers.\n",
        "\n",
        "Reinforcement Learning with Human Feedback (RLHF)\n",
        "Integrating reinforcement learning principles with human feedback:\n",
        "\n",
        "Reinforcement Learning (RL): Maximizing cumulative reward through interaction with an environment.\n",
        "\n",
        "Human Feedback Integration: Incorporating human-provided feedback into the RL loop.\n",
        "\n",
        "Reward Modeling: Formulating human feedback as rewards for the RL agent.\n",
        "Imitation Learning: Learning from human demonstrations to bootstrap training.\n",
        "Inverse Reinforcement Learning: Inferring reward functions from observed behavior, including human demonstrations."
      ],
      "metadata": {
        "id": "QhZF5Ls-NxD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 FUNDAMENTAL MATHEMATICS BEHIND GENRATIVE AI"
      ],
      "metadata": {
        "id": "t5PyX0QhqpHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what is Mathematics?\n",
        "Mathematics a symbolic representation of reality. While designing any solution (software or engineering) there is an early process of desining and validation. during this stage we need a language in which we can represnt the state, flow, input and outputs. Math is that language to represnt reality and further it is used to validate the output in theory without actually creating anything."
      ],
      "metadata": {
        "id": "44r97JKqq1eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is differentiation?\n",
        "Differentiation is a process of doing comparative evaluations i.e. rate of change. In differentiation we are calculating how will y change when another related variable x changes for example Change in Salary as measured to the change in education or Change in IQ as measured to the change in Learning (i am sure its inversly related :)). If we plot both the y and the x variable on a 2d plane, there will be a point in the plot where a tangent to the plot will be flat or 0. at this point the differentaion of y over x will be zero. This is the point which will be either maximum or minimum. Our training objective for the models is to find this point and adjust parameters which are best to reach this point.\n",
        "Optimizing for a minimum: To find the minimum of a function, you'd use methods like gradient descent. It involves starting at a point, then moving in the direction of the steepest descent (opposite the gradient) iteratively until you reach a minimum.\n",
        "\n",
        "Optimizing for a maximum: Similarly, to find the maximum, you can use gradient ascent. This method involves moving in the direction of the steepest ascent (in the direction of the gradient) iteratively until you reach a maximum."
      ],
      "metadata": {
        "id": "ndjTMhs4rfcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent:\n",
        "Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of the steepest descent (negative gradient) of the function. It's commonly applied in machine learning for training models by adjusting parameters to minimize a cost function. Please refer to the attached excel and video narration on Gradient Descent.\n"
      ],
      "metadata": {
        "id": "w4gotsCXPmlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function\n",
        "In machine learning, a loss function acts as a guide for evaluating a model's performance by quantifying the disparity between predicted outcomes and actual values within a dataset. Its primary purpose is to measure how 'off' the model's predictions are, providing a single numerical value that represents the magnitude of the error. The choice of the loss function heavily depends on the nature of the problem being addressed; for instance, Mean Squared Error (MSE) serves regression tasks, calculating the average of squared differences between predicted and actual values, while Cross-Entropy Loss (or Log Loss) is commonly applied in classification problems to evaluate probabilities. Hinge Loss finds utility in support vector machines for classification, whereas Mean Absolute Error (MAE) computes the average of absolute differences in regression. During model training, the objective revolves around minimizing this loss function, achieved by adjusting the model's parameters through techniques like gradient descent. The iterative process of optimization aims to find the most suitable set of parameters that minimize the chosen loss function, consequently enhancing the model's predictive accuracy. Additionally, the evaluation and improvement of models hinge on the observation of how the loss changes throughout training and testing phases. This observation informs potential modifications to the model, such as altering architecture, adjusting hyperparameters, or refining the dataset, ultimately contributing to improved model performance and reliability in real-world applications. Therefore, the selection and design of an appropriate loss function are critical elements in constructing successful machine learning models across diverse problem domains."
      ],
      "metadata": {
        "id": "DHr8EC2oSAfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 DEEP LEARNING FUNDAMENTALS"
      ],
      "metadata": {
        "id": "dx7BU9EBn3ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1 WHAT IS A NEURAL NETWORK?\n",
        "A neural network is a computational model inspired by the human brain's structure and functioning. It consists of interconnected nodes, called neurons, organized into layers. Each neuron processes input data, applies weights and biases, and passes the output to the next layer, eventually producing a final output.\n",
        "\n",
        "Let's consider a simple example of a neural network used for image classification:\n",
        "\n",
        "Suppose we have a neural network designed to recognize handwritten digits (0-9). This network has three layers: an input layer, a hidden layer, and an output layer.\n",
        "\n",
        "Input Layer: It receives pixel values of an image representing a handwritten digit. Each pixel value acts as a separate input neuron.\n",
        "\n",
        "Hidden Layer: Neurons in this layer process the input data by applying weights and biases, performing mathematical operations, and passing the output to the next layer. The hidden layer helps the network learn complex patterns and features within the input data.\n",
        "\n",
        "Output Layer: It produces the final result, indicating the probability or confidence score of the input digit belonging to each possible class (0-9).\n",
        "\n",
        "During the training phase, the neural network learns by adjusting its weights and biases to minimize the difference between its predictions and the actual labels of the input data. This process involves forward propagation (passing data through the network to get predictions) and backward propagation (adjusting weights based on prediction errors using techniques like gradient descent).\n",
        "\n",
        "Once trained, the neural network can accurately classify handwritten digits by taking an image as input, processing it through the layers, and producing a probability distribution over the possible classes, indicating the digit it most likely represents."
      ],
      "metadata": {
        "id": "86iPHPcPn-oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2 WHAT IS FORWARD AND BACKWARD PROPAGATION\n",
        "Forward Propagation:\n",
        "Forward propagation is the process in which data is passed through the neural network, layer by layer, to generate predictions or outputs.\n",
        "Each neuron in a layer takes inputs, performs computations (like weighted sum and activation function), and passes the result to the neurons in the subsequent layer.\n",
        "The inputs are multiplied by weights, summed up, and then passed through an activation function to produce the neuron's output.\n",
        "This process continues through the network, from the input layer to the output layer, producing the final predictions.\n",
        "The key idea is that information flows forward through the network, layer by layer, to generate predictions.\n",
        "\n",
        "Backward Propagation:\n",
        "Backward propagation (also known as backpropagation) is the process used to compute gradients of the loss function with respect to the weights and biases in the network.\n",
        "Once predictions are made, the computed loss (difference between predicted and actual values) is calculated.\n",
        "\n",
        "Backpropagation involves working backward through the network to determine how much each neuron's weights and biases contributed to the overall error.\n",
        "It computes gradients of the loss function with respect to the weights and biases by applying the chain rule of calculus, starting from the output layer and moving backward through the network.\n",
        "These gradients guide the optimization algorithm (like gradient descent) to update the weights and biases, aiming to minimize the loss function.\n",
        "By iteratively adjusting weights and biases based on these gradients, the model learns to improve its predictions.\n",
        "In simpler terms, forward propagation is the forward flow of data through the network to make predictions, while backward propagation is the backward flow of error information to update the network's parameters (weights and biases) for better predictions in the future.\n",
        "Lets look at an example below, in this example This example demonstrates forward propagation, where predictions are made using initial weights and bias. Then, backpropagation computes gradients using the loss function, and weights and bias are updated using gradient descent to minimize the loss. Finally, we plot the predictions against the actual data."
      ],
      "metadata": {
        "id": "Vb6p6XckZQDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(50, 1) * 10\n",
        "y = 2 * X + 3 + np.random.randn(50, 1)  # y = 2X + 3 + noise\n",
        "\n",
        "# Initialize weights and bias with random values\n",
        "weights = np.random.randn(1)\n",
        "bias = np.random.randn(1)\n",
        "\n",
        "# Define forward propagation\n",
        "def forward_propagation(x):\n",
        "    return x * weights + bias\n",
        "\n",
        "# Define loss function (MSE)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Define learning rate and number of epochs for training\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Training loop (forward and backward propagation)\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    predictions = forward_propagation(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(y, predictions)\n",
        "\n",
        "    # Backpropagation\n",
        "    gradient_weights = -2 * np.mean(X * (y - predictions))\n",
        "    gradient_bias = -2 * np.mean(y - predictions)\n",
        "\n",
        "    # Update weights and bias using gradient descent\n",
        "    weights -= learning_rate * gradient_weights\n",
        "    bias -= learning_rate * gradient_bias\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss}\")\n",
        "\n",
        "# Plotting the final prediction\n",
        "plt.scatter(X, y, label='Actual data')\n",
        "plt.plot(X, forward_propagation(X), color='red', label='Predictions')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Regression Predictions')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NwS6ql9TZx9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 NODES:\n",
        "A node or neuron in a network is a computation unit that processes input data, the following code is an abstract representation"
      ],
      "metadata": {
        "id": "zyz1e5qLonWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  def __init__(self):\n",
        "    self.input_data = None\n",
        "    self.output = None\n",
        "  def forward(self, input_data):\n",
        "    # Process input data (perform computations)\n",
        "    self.input_data = input_data\n",
        "    # Compute Output\n",
        "    self.output = some_function(input_data)\n",
        "    return self.output"
      ],
      "metadata": {
        "id": "QLd3nAJuo3F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.3 EDGE\n",
        "An Edge represents the connection between nodes and carries a weight that affects the signal passed between them. Here is a conceptual representation"
      ],
      "metadata": {
        "id": "J6mZnlX6pbPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Edge:\n",
        "  def __init__(self, weight):\n",
        "    self.weight = weight"
      ],
      "metadata": {
        "id": "0_Ap_sesp2k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4  INPUT LAYER\n",
        "The input layer receives input data and passes it to the next layer. In code, using Keras from TensorFlow:"
      ],
      "metadata": {
        "id": "R_RKXIA3UCGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Input\n",
        "\n",
        "input_layer = Input(shape=(input_shape,))"
      ],
      "metadata": {
        "id": "zuqa83T4UOvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.5 DENSE LAYER\n",
        "A dense layer (fully connected layer) connects every neuron in the current layer to every neuron in the next layer. For example this can be the layer which is between the input and the Output Layers. Following code defines a dense layer"
      ],
      "metadata": {
        "id": "yzYpmNNyUT-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense_layer = layers.Dense(units=128, activation='relu')"
      ],
      "metadata": {
        "id": "hItCRgfUUlFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.5 DROPOUT LAYER\n",
        "Dropout is a regularization technique that randomly sets a fraction of input units to 0 during training to prevent overfitting. Here is the code for a dropout layer"
      ],
      "metadata": {
        "id": "NH8Jn8w8Untt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_layer = layers.Dropout(rate=0.2)"
      ],
      "metadata": {
        "id": "NsYgNE0hUzuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 LAYER NORMALIZATION\n",
        "Imagine you have a set of three numbers representing inputs to a neural network's hidden layer: 10, 20, and 30.\n",
        "\n",
        "Step 1: Calculate Mean and Variance\n",
        "\n",
        "Mean (Average): Add all the numbers together and divide by how many numbers there are.\n",
        "\n",
        "For 10, 20, 30:\n",
        "\n",
        "Average = (10 + 20 + 30) / 3 = 60 / 3 = 20\n",
        "\n",
        "Variance (How spread out the numbers are from the mean):\n",
        "\n",
        "Calculate the squared differences from the mean, sum them, and then divide by how many numbers there are.\n",
        "\n",
        "Variance = [(10 - 20)^2 + (20 - 20)^2 + (30 - 20)^2] / 3\n",
        "\n",
        "Variance = [(100) + (0) + (100)] / 3 = 200 / 3\n",
        "\n",
        "Step 2: Apply Layer Normalization\n",
        "\n",
        "For each number in the input:\n",
        "Normalize each number by subtracting the mean and dividing by the square root of the variance plus a small value (epsilon) to avoid division by zero.\n",
        "\n",
        "LN(10) = (10 - 20) / square root of (200/3 + epsilon)\n",
        "\n",
        "LN(20) = (20 - 20) / square root of (200/3 + epsilon) = 0\n",
        "\n",
        "LN(30) = (30 - 20) / square root of (200/3 + epsilon)\n",
        "Layer Normalization adjusts the values by considering the average (mean) of all the numbers and how spread out (variance) they are from that average. This process helps the neural network to handle different scales of input values and makes learning more stable during training.\n"
      ],
      "metadata": {
        "id": "jCHOSUAnU2K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_norm = layers.LayerNormalization(axis=-1)"
      ],
      "metadata": {
        "id": "Pmf0e-6gWZAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.7 TENSOR\n",
        "A tensor is a multi-dimensional array used to represent data in neural networks.\n",
        "Google created TensorFlow, which heavily utilizes tensors for its computations and operations. Tensors are represented using a combination of arrays, where each axis of the array represents a different dimension. TensorFlow, being designed for efficient numerical computations, uses tensors to efficiently handle and process large amounts of data for machine learning tasks."
      ],
      "metadata": {
        "id": "63FCnRnLWbzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Creating tensors in TensorFlow\n",
        "# 0-dimensional tensor (scalar)\n",
        "scalar_tensor = tf.constant(5)\n",
        "print(\"Scalar Tensor:\")\n",
        "print(scalar_tensor)\n",
        "\n",
        "# 1-dimensional tensor (vector)\n",
        "vector_tensor = tf.constant([1, 2, 3, 4, 5])\n",
        "print(\"\\nVector Tensor:\")\n",
        "print(vector_tensor)\n",
        "\n",
        "# 2-dimensional tensor (matrix)\n",
        "matrix_tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"\\nMatrix Tensor:\")\n",
        "print(matrix_tensor)\n",
        "\n",
        "# 3-dimensional tensor\n",
        "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(\"\\n3D Tensor:\")\n",
        "print(tensor_3d)\n"
      ],
      "metadata": {
        "id": "twJCvYAjWkzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.8 VECTORIZATION\n",
        "Vectorization refers to performing operations on entire arrays rather than iterating through elements, which can significantly speed up computations in neural networks."
      ],
      "metadata": {
        "id": "G7x6mJX8WnOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of vectorized operation in NumPy\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "# Vectorized addition\n",
        "result = a + b"
      ],
      "metadata": {
        "id": "l8f9JstWWxbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.9 ACTIVATION FUNCTION\n",
        "The activation function in a neural network serves two primary purposes:\n",
        "\n",
        "Introduction of Non-Linearity: Activation functions introduce non-linear properties to the network, allowing it to learn and approximate complex relationships between inputs and outputs. Without non-linear activation functions, even a deep neural network would behave like a single-layer model because stacking linear operations results in another linear operation.\n",
        "\n",
        "Decision-Making: Activation functions determine whether a neuron should be activated or not, depending on whether the input passes a certain threshold. This helps in introducing non-linear transformations to the inputs and enables the network to learn and represent more intricate patterns and relationships in the data.\n",
        "\n",
        "Impact on a Node:\n",
        "The activation function operates on the weighted sum of inputs plus a bias term within a node (neuron) of the neural network. It decides whether the neuron should be activated or not based on the result of this operation. The output of the activation function becomes the input for the next layer of the network.\n",
        "\n",
        "Different types of activation functions have distinct characteristics and affect nodes differently:\n",
        "\n",
        "Linear Activation Function: This function produces a straight line with a slope, and its impact is linear. It's not commonly used in hidden layers because stacking linear functions doesn't enhance the network's learning capacity.\n",
        "\n",
        "Non-linear Activation Functions (e.g., ReLU, Sigmoid, Tanh): These introduce non-linearities to the network, enabling it to learn complex patterns. For instance:\n",
        "\n",
        "ReLU (Rectified Linear Unit): Sets negative inputs to zero and keeps positive inputs unchanged, introducing non-linearity by allowing only positive values to pass.\n",
        "Sigmoid: Squashes the input values between 0 and 1, enabling the neuron to simulate a binary output (0 or 1).\n",
        "Tanh (Hyperbolic Tangent): Similar to sigmoid, but squashes values between -1 and 1, providing a broader range for output values compared to sigmoid.\n",
        "The choice of activation function impacts the network's learning capabilities, training speed, convergence, and ability to handle different types of data. The selection depends on the specific problem being solved and the characteristics of the data."
      ],
      "metadata": {
        "id": "fxNLvCPdW0L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of activation function (ReLU) in TensorFlow\n",
        "activation = layers.Activation('relu')"
      ],
      "metadata": {
        "id": "9w0fJpsmXTnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.10 WHAT ARE WEIGHTS, WHERE ARE THEY SAVED AFTER UPDATION?\n",
        "In a neural network, each node (neuron) typically stores its weights and biases as parameters. These parameters are updated during the training process through techniques like backpropagation and gradient descent. Each input to a node is associated with a weight, and the node computes a weighted sum of its inputs using these weights.\n",
        "\n",
        "Let's create a simple Node class in Python to demonstrate how weights can be stored within a node. In this example\n",
        "\n",
        "The Node class initializes its weights and bias when created.\n",
        "\n",
        "The compute_output method calculates the weighted sum of inputs using the stored weights and adds the bias.\n",
        "\n",
        "The update_weights method allows updating the weights of the node with a new set of weights."
      ],
      "metadata": {
        "id": "C-WX5W8QXpA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, num_inputs):\n",
        "        # Initialize weights randomly\n",
        "        self.weights = [0.1 for _ in range(num_inputs)]  # For demonstration, initializing all weights to 0.1\n",
        "        self.bias = 0.0\n",
        "\n",
        "    def compute_output(self, inputs):\n",
        "        # Perform weighted sum of inputs and weights, add bias\n",
        "        weighted_sum = sum(x * w for x, w in zip(inputs, self.weights)) + self.bias\n",
        "        return weighted_sum\n",
        "\n",
        "    def update_weights(self, new_weights):\n",
        "        # Update the weights of the node\n",
        "        if len(new_weights) == len(self.weights):\n",
        "            self.weights = new_weights\n",
        "            print(\"Weights updated successfully.\")\n",
        "        else:\n",
        "            print(\"Number of new weights doesn't match the existing weights.\")"
      ],
      "metadata": {
        "id": "xQGtMVoeX66E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's create an instance of this Node class and demonstrate its functionality:"
      ],
      "metadata": {
        "id": "6GAzvHUmYQUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a node with 3 inputs\n",
        "node = Node(3)\n",
        "\n",
        "# Example inputs\n",
        "inputs = [1, 2, 3]\n",
        "\n",
        "# Compute the output using the initialized weights and inputs\n",
        "output = node.compute_output(inputs)\n",
        "print(\"Output before weight update:\", output)\n",
        "\n",
        "# Update the weights of the node\n",
        "new_weights = [0.2, 0.3, 0.4]\n",
        "node.update_weights(new_weights)\n",
        "\n",
        "# Compute the output using the updated weights and inputs\n",
        "output_after_update = node.compute_output(inputs)\n",
        "print(\"Output after weight update:\", output_after_update)\n"
      ],
      "metadata": {
        "id": "9Z1OxfVSYR9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a neural network, weights are saved as parameters within each layer of the network. During the training process, these weights are adjusted to optimize the network's performance on the given task. Once trained, the optimized weights are stored within the model.\n",
        "\n",
        "For instance, if you're using a deep learning framework like TensorFlow or PyTorch, the weights are typically stored as attributes within the layers or modules of the model.\n",
        "\n",
        "Let's consider a simple example using TensorFlow to create a neural network with dense layers:"
      ],
      "metadata": {
        "id": "dz_flvm2YUY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a simple neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, input_shape=(10,), activation='relu'),  # Input layer with 10 inputs and 64 neurons\n",
        "    tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer with 32 neurons\n",
        "    tf.keras.layers.Dense(1)  # Output layer with 1 neuron (regression example)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Display the summary of the model architecture\n",
        "model.summary()\n",
        "\n",
        "# Get the weights of the first layer\n",
        "first_layer_weights = model.layers[0].get_weights()\n",
        "print(\"Weights of the first layer:\")\n",
        "print(first_layer_weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "CNjcncpjYc4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 TRANSFORMER MODEL WITH ATTENTION"
      ],
      "metadata": {
        "id": "GlgKhADDa2Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXDShK6tauyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1  "
      ],
      "metadata": {
        "id": "_0hgDd7sa9UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 BUILDING TRANSFORMER MODELS WITH SELF ATTENTION FROM SCRATCH"
      ],
      "metadata": {
        "id": "bfjQs_0tEXd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1 Import Base Libraries\n"
      ],
      "metadata": {
        "id": "oGWcatutEa2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quKAfe7tEKjK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2 Self-Attention Mechanism Layer"
      ],
      "metadata": {
        "id": "IxJ1AqSsEre6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = self.add_weight(name='W_q',\n",
        "                                   shape=(input_shape[-1], self.embed_dim),\n",
        "                                   initializer='glorot_uniform',\n",
        "                                   trainable=True)\n",
        "        self.W_k = self.add_weight(name='W_k',\n",
        "                                   shape=(input_shape[-1], self.embed_dim),\n",
        "                                   initializer='glorot_uniform',\n",
        "                                   trainable=True)\n",
        "        self.W_v = self.add_weight(name='W_v',\n",
        "                                   shape=(input_shape[-1], self.embed_dim),\n",
        "                                   initializer='glorot_uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = tf.matmul(inputs, self.W_q)\n",
        "        k = tf.matmul(inputs, self.W_k)\n",
        "        v = tf.matmul(inputs, self.W_v)\n",
        "\n",
        "        attn_scores = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_scores = tf.nn.softmax(attn_scores / tf.math.sqrt(tf.cast(self.embed_dim, tf.float32)), axis=-1)\n",
        "        output = tf.matmul(attn_scores, v)\n",
        "        return output"
      ],
      "metadata": {
        "id": "L8XxmJSAEkLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.3 Transformer Model"
      ],
      "metadata": {
        "id": "HNZedncTEz2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_layers, vocab_size, max_length):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Positional Encoding\n",
        "    position_embed = np.array([[pos / np.power(10000, 2 * (i // 2) / embed_dim) for i in range(embed_dim)]\n",
        "                               for pos in range(max_length)])\n",
        "    position_embed[:, 0::2] = np.sin(position_embed[:, 0::2])\n",
        "    position_embed[:, 1::2] = np.cos(position_embed[:, 1::2])\n",
        "    position_embed = tf.convert_to_tensor(position_embed, dtype=tf.float32)\n",
        "    pos_encoding = position_embed[:input_shape[1], :]\n",
        "    x = inputs + pos_encoding\n",
        "\n",
        "    # Transformer Encoder\n",
        "    for _ in range(num_layers):\n",
        "        attn_output = SelfAttention(embed_dim)(x)\n",
        "        attn_output = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "        ffn_output = Dense(ff_dim, activation='relu')(attn_output)\n",
        "        ffn_output = Dense(embed_dim)(ffn_output)\n",
        "        x = LayerNormalization(epsilon=1e-6)(ffn_output + attn_output)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "tBpN67IAE3mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.4 Example parameters"
      ],
      "metadata": {
        "id": "U_Hxo75KE75b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (100, 512)  # Input shape: (sequence_length, embedding_dimension)\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "num_layers = 4\n",
        "vocab_size = 10000\n",
        "max_length = 1000"
      ],
      "metadata": {
        "id": "E2nZuwrHE_15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.5 Build Transformer Model"
      ],
      "metadata": {
        "id": "wkGxGrFbFEFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = build_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_layers, vocab_size, max_length)\n",
        "transformer_model.summary()"
      ],
      "metadata": {
        "id": "WXZOlFBqFJ7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 PLAYAREA"
      ],
      "metadata": {
        "id": "5XFtzP7LVSTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "OOXd1apKVQdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, input_shape=(10,), activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "model.compile(optimizer='adam', loss = 'mean_squared_error')\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "vjNDbVBfVdLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_layer_weights = model.layers[0].get_weights()\n",
        "print(\"weights of the first layer:\")\n",
        "print(first_layer_weights)"
      ],
      "metadata": {
        "id": "QZiBc6-yWpEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QaUJF8_9XGcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}